{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c90b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplotot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590ed777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max.colwidth',150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c079e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['SENTIMENT', 'REVIEW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc53ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('emergencyplus_combined_sentiment_basic_clean.csv',names=colnames, header=None, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ed0c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    528\n",
       "negative    249\n",
       "neutral     177\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SENTIMENT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2633cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REMOVE PUNCTUATUIOMS\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f50a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203c0055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct.</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...</td>\n",
       "      <td>Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>A good thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We</td>\n",
       "      <td>We</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0  negative   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3   neutral   \n",
       "4   neutral   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                              Gives wrong address so how do I know the rest is correct.   \n",
       "1  Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                                      REVIEW_punctuation  \n",
       "0                                                                                               Gives wrong address so how do I know the rest is correct  \n",
       "1  Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...  \n",
       "2                                                                                                                                       A good thank you  \n",
       "3                                                                                                                                                     We  \n",
       "4                                                                                                                                          handy to have  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['REVIEW_punctuation'] = df['REVIEW'].apply(lambda x: remove_punctuation(x))\n",
    "df.head()\n",
    "#It is just a list of characters, so i added \"\".join to the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cc05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELLOW CODE IS FOR TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407fbad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_punctuation</th>\n",
       "      <th>REVIEW_punctuation_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct.</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct</td>\n",
       "      <td>[gives, wrong, address, so, how, do, i, know, the, rest, is, correct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...</td>\n",
       "      <td>Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...</td>\n",
       "      <td>[locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>[a, good, thank, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We</td>\n",
       "      <td>We</td>\n",
       "      <td>[we]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>[handy, to, have]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0  negative   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3   neutral   \n",
       "4   neutral   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                              Gives wrong address so how do I know the rest is correct.   \n",
       "1  Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                                      REVIEW_punctuation  \\\n",
       "0                                                                                               Gives wrong address so how do I know the rest is correct   \n",
       "1  Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                            REVIEW_punctuation_tokenized  \n",
       "0                                                                                  [gives, wrong, address, so, how, do, i, know, the, rest, is, correct]  \n",
       "1  [locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...  \n",
       "2                                                                                                                                  [a, good, thank, you]  \n",
       "3                                                                                                                                                   [we]  \n",
       "4                                                                                                                                      [handy, to, have]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#\\W for nonword characters? and \\w for word characters? also done lowercase\n",
    "def tokenize(txt):\n",
    "    tokens = re.split('\\W+', txt)\n",
    "    return tokens\n",
    "#we make a new column to store clean and tokenized with all lower case\n",
    "df['REVIEW_punctuation_tokenized'] = df['REVIEW_punctuation'].apply(lambda x:tokenize(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "959fe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fda5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bbed0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05efda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_punctuation</th>\n",
       "      <th>REVIEW_punctuation_tokenized</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct.</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct</td>\n",
       "      <td>[gives, wrong, address, so, how, do, i, know, the, rest, is, correct]</td>\n",
       "      <td>[gives, wrong, address, know, rest, correct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...</td>\n",
       "      <td>Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...</td>\n",
       "      <td>[locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...</td>\n",
       "      <td>[locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>[a, good, thank, you]</td>\n",
       "      <td>[good, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We</td>\n",
       "      <td>We</td>\n",
       "      <td>[we]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>[handy, to, have]</td>\n",
       "      <td>[handy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0  negative   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3   neutral   \n",
       "4   neutral   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                              Gives wrong address so how do I know the rest is correct.   \n",
       "1  Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                                      REVIEW_punctuation  \\\n",
       "0                                                                                               Gives wrong address so how do I know the rest is correct   \n",
       "1  Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                            REVIEW_punctuation_tokenized  \\\n",
       "0                                                                                  [gives, wrong, address, so, how, do, i, know, the, rest, is, correct]   \n",
       "1  [locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...   \n",
       "2                                                                                                                                  [a, good, thank, you]   \n",
       "3                                                                                                                                                   [we]   \n",
       "4                                                                                                                                      [handy, to, have]   \n",
       "\n",
       "                                                                                                                         REVIEW_punctuation_tokenized_sw  \n",
       "0                                                                                                           [gives, wrong, address, know, rest, correct]  \n",
       "1  [locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]  \n",
       "2                                                                                                                                          [good, thank]  \n",
       "3                                                                                                                                                     []  \n",
       "4                                                                                                                                                [handy]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(txt_tokenized):\n",
    "    txt_clean = [word for word in txt_tokenized if word not in stopwords]\n",
    "    return txt_clean\n",
    "#Creating a new column to put the reviews without stop words in\n",
    "df['REVIEW_punctuation_tokenized_sw'] = df['REVIEW_punctuation_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb0463e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec7b28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300ef7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coder\n",
      "code\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "#just testing\n",
    "print(ps.stem('coder'))\n",
    "print(ps.stem('coding'))\n",
    "print(ps.stem('code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8fcce2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "datum\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "#just testing\n",
    "print(ps.stem('data'))\n",
    "print(ps.stem('datum'))\n",
    "print(ps.stem('code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "150b80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Process of our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee07632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58633342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_punctuation</th>\n",
       "      <th>REVIEW_punctuation_tokenized</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct.</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct</td>\n",
       "      <td>[gives, wrong, address, so, how, do, i, know, the, rest, is, correct]</td>\n",
       "      <td>[gives, wrong, address, know, rest, correct]</td>\n",
       "      <td>[give, wrong, address, know, rest, correct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...</td>\n",
       "      <td>Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...</td>\n",
       "      <td>[locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...</td>\n",
       "      <td>[locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]</td>\n",
       "      <td>[locat, correctli, give, address, 800m, 8, street, away, last, updat, text, also, odd, ie, 19, year, ago, must, let, take, time, settl, sgs10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>[a, good, thank, you]</td>\n",
       "      <td>[good, thank]</td>\n",
       "      <td>[good, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We</td>\n",
       "      <td>We</td>\n",
       "      <td>[we]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>[handy, to, have]</td>\n",
       "      <td>[handy]</td>\n",
       "      <td>[handi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0  negative   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3   neutral   \n",
       "4   neutral   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                              Gives wrong address so how do I know the rest is correct.   \n",
       "1  Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                                      REVIEW_punctuation  \\\n",
       "0                                                                                               Gives wrong address so how do I know the rest is correct   \n",
       "1  Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                            REVIEW_punctuation_tokenized  \\\n",
       "0                                                                                  [gives, wrong, address, so, how, do, i, know, the, rest, is, correct]   \n",
       "1  [locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...   \n",
       "2                                                                                                                                  [a, good, thank, you]   \n",
       "3                                                                                                                                                   [we]   \n",
       "4                                                                                                                                      [handy, to, have]   \n",
       "\n",
       "                                                                                                                         REVIEW_punctuation_tokenized_sw  \\\n",
       "0                                                                                                           [gives, wrong, address, know, rest, correct]   \n",
       "1  [locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]   \n",
       "2                                                                                                                                          [good, thank]   \n",
       "3                                                                                                                                                     []   \n",
       "4                                                                                                                                                [handy]   \n",
       "\n",
       "                                                                                                          REVIEW_punctuation_tokenized_sw_stemmed  \n",
       "0                                                                                                     [give, wrong, address, know, rest, correct]  \n",
       "1  [locat, correctli, give, address, 800m, 8, street, away, last, updat, text, also, odd, ie, 19, year, ago, must, let, take, time, settl, sgs10]  \n",
       "2                                                                                                                                   [good, thank]  \n",
       "3                                                                                                                                              []  \n",
       "4                                                                                                                                         [handi]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['REVIEW_punctuation_tokenized_sw_stemmed'] = df['REVIEW_punctuation_tokenized_sw'].apply(lambda x:stemming(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "21f2b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Lemmatizer / WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c4a703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11ff3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_text):\n",
    "    text=[wn.lemmatize(word) for word in token_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "754a7fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_punctuation</th>\n",
       "      <th>REVIEW_punctuation_tokenized</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw_stemmed</th>\n",
       "      <th>REVIEW_punctuation_tokenized_sw_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct.</td>\n",
       "      <td>Gives wrong address so how do I know the rest is correct</td>\n",
       "      <td>[gives, wrong, address, so, how, do, i, know, the, rest, is, correct]</td>\n",
       "      <td>[gives, wrong, address, know, rest, correct]</td>\n",
       "      <td>[give, wrong, address, know, rest, correct]</td>\n",
       "      <td>[give, wrong, address, know, rest, correct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...</td>\n",
       "      <td>Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...</td>\n",
       "      <td>[locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...</td>\n",
       "      <td>[locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]</td>\n",
       "      <td>[locat, correctli, give, address, 800m, 8, street, away, last, updat, text, also, odd, ie, 19, year, ago, must, let, take, time, settl, sgs10]</td>\n",
       "      <td>[locates, correctly, give, address, 800m, 8, street, away, last, updated, text, also, odd, ie, 19, year, ago, must, let, take, time, settle, sgs10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>A good thank you</td>\n",
       "      <td>[a, good, thank, you]</td>\n",
       "      <td>[good, thank]</td>\n",
       "      <td>[good, thank]</td>\n",
       "      <td>[good, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We</td>\n",
       "      <td>We</td>\n",
       "      <td>[we]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>[handy, to, have]</td>\n",
       "      <td>[handy]</td>\n",
       "      <td>[handi]</td>\n",
       "      <td>[handy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0  negative   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3   neutral   \n",
       "4   neutral   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                              Gives wrong address so how do I know the rest is correct.   \n",
       "1  Locates correctly but can give an address 800m / 8 streets away. \"Last updated\" text can also be odd: ie \"19 years ago\". Must have to let it take ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                                      REVIEW_punctuation  \\\n",
       "0                                                                                               Gives wrong address so how do I know the rest is correct   \n",
       "1  Locates correctly but can give an address 800m  8 streets away Last updated text can also be odd ie 19 years ago Must have to let it take time to ...   \n",
       "2                                                                                                                                       A good thank you   \n",
       "3                                                                                                                                                     We   \n",
       "4                                                                                                                                          handy to have   \n",
       "\n",
       "                                                                                                                            REVIEW_punctuation_tokenized  \\\n",
       "0                                                                                  [gives, wrong, address, so, how, do, i, know, the, rest, is, correct]   \n",
       "1  [locates, correctly, but, can, give, an, address, 800m, 8, streets, away, last, updated, text, can, also, be, odd, ie, 19, years, ago, must, have,...   \n",
       "2                                                                                                                                  [a, good, thank, you]   \n",
       "3                                                                                                                                                   [we]   \n",
       "4                                                                                                                                      [handy, to, have]   \n",
       "\n",
       "                                                                                                                         REVIEW_punctuation_tokenized_sw  \\\n",
       "0                                                                                                           [gives, wrong, address, know, rest, correct]   \n",
       "1  [locates, correctly, give, address, 800m, 8, streets, away, last, updated, text, also, odd, ie, 19, years, ago, must, let, take, time, settle, sgs10]   \n",
       "2                                                                                                                                          [good, thank]   \n",
       "3                                                                                                                                                     []   \n",
       "4                                                                                                                                                [handy]   \n",
       "\n",
       "                                                                                                          REVIEW_punctuation_tokenized_sw_stemmed  \\\n",
       "0                                                                                                     [give, wrong, address, know, rest, correct]   \n",
       "1  [locat, correctli, give, address, 800m, 8, street, away, last, updat, text, also, odd, ie, 19, year, ago, must, let, take, time, settl, sgs10]   \n",
       "2                                                                                                                                   [good, thank]   \n",
       "3                                                                                                                                              []   \n",
       "4                                                                                                                                         [handi]   \n",
       "\n",
       "                                                                                                            REVIEW_punctuation_tokenized_sw_lemmatized  \n",
       "0                                                                                                          [give, wrong, address, know, rest, correct]  \n",
       "1  [locates, correctly, give, address, 800m, 8, street, away, last, updated, text, also, odd, ie, 19, year, ago, must, let, take, time, settle, sgs10]  \n",
       "2                                                                                                                                        [good, thank]  \n",
       "3                                                                                                                                                   []  \n",
       "4                                                                                                                                              [handy]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['REVIEW_punctuation_tokenized_sw_lemmatized'] = df['REVIEW_punctuation_tokenized_sw'].apply(lambda x:lemmatization(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b7afb02",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.to_csv(\"clean_emergencyplus_combind_allMethods.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88026fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code is valid up to here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f12fc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the text into the vector\n",
    "#type of vectorization:\n",
    "#count vectorization\n",
    "#n-grams\n",
    "#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1b8264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b1cfd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d18cb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dc9ab87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 6, 'is': 3, 'sentence': 4, 'another': 0, 'third': 5, 'document': 1, 'here': 2}\n",
      "['another' 'document' 'here' 'is' 'sentence' 'third' 'this']\n",
      "(3, 7)\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "[[0 0 0 2 1 0 1]\n",
      " [1 0 0 1 1 0 1]\n",
      " [0 1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#testing an example (will be deleted)\n",
    "corpus = [\"this is a sentence is\", \"this is another sentence\", \"third document is here\"]\n",
    "x = cv.fit(corpus)\n",
    "print(x.vocabulary_)\n",
    "print(cv.get_feature_names_out())\n",
    "x = cv.transform(corpus)\n",
    "x=cv.fit_transform(corpus)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "#to print a mattrix we use the bellow code\n",
    "print(x.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8753c98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   another  document  here  is  sentence  third  this\n",
      "0        0         0     0   2         1      0     1\n",
      "1        1         0     0   1         1      0     1\n",
      "2        0         1     1   1         0      1     0\n"
     ]
    }
   ],
   "source": [
    "#Part of the above example (will be deleted later)\n",
    "dftest = pd.DataFrame(x.toarray(), columns = cv.get_feature_names_out())\n",
    "print(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "72703432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning shortend in one function \n",
    "def clean_text(txt):\n",
    "    txt=\"\".join([c for c in txt if c not in string.punctuation])\n",
    "    tokens = re.split('\\W+' , txt)\n",
    "    txt = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "676d34f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(751, 1759)\n"
     ]
    }
   ],
   "source": [
    "#CountVectorization on main dat\n",
    "cv = CountVectorizer()\n",
    "cv1 = CountVectorizer(analyzer=clean_text)\n",
    "#The bellow code returns an error, (why?) thats why I replaced it with another code\n",
    "# x = cv1.fit_transform(df['REVIEW_clean_tokenized_no_sw_lemmatized'])\n",
    "x1 = cv1.fit_transform(df['REVIEW'])\n",
    "print(x.shape)\n",
    "print(x1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "13db8078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '0', '000', '1', '10', '100', '106', '10and', '10m', '11', '11112020', '112', '12', '121120', '129', '13', '13112020', '141120', '14433854', '15', '150m', '157', '159', '161120', '18', '18nov2020', '194', '1s', '1star', '2', '20', '2013', '2016', '2020', '20m', '20th', '22', '223', '224', '23', '25', '26', '2km', '2nov2020', '3', '30', '3234m', '35', '36', '3667637', '3eff', '3g', '3km', '3m', '3rd', '3word', '4', '45km', '47', '4g', '5', '500m', '50km', '52', '5km', '6', '601', '630', '67', '6p', '7', '70', '700metr', '711', '750m', '8', '800', '81', '84', '850', '9', '9am', 'a', 'a3e', 'a5', 'a9', 'abil', 'abl', 'absolut', 'abus', 'access', 'accid', 'accompani', 'accord', 'accur', 'accuraci', 'ace', 'across', 'action', 'activ', 'actual', 'ad', 'add', 'addit', 'address', 'adjoin', 'adjust', 'adopt', 'adrenalin', 'adress', 'advanc', 'advertis', 'advic', 'advis', 'afer', 'affect', 'afl', 'againgreat', 'age', 'agenc', 'aggressor', 'ago', 'agre', 'agreement', 'aid', 'aint', 'aircrew', 'airplan', 'aiton', 'ala', 'alarm', 'alert', 'alic', 'alik', 'allergi', 'allig', 'allow', 'almost', 'alon', 'along', 'alpha', 'alreadi', 'also', 'alter', 'although', 'alway', 'amaz', 'ambo', 'ambul', 'ancient', 'andor', 'android', 'anim', 'announc', 'anoth', 'anyin', 'anymor', 'anyon', 'anyth', 'anyway', 'anywayani', 'anywher', 'apart', 'api', 'apk', 'app', 'appar', 'appear', 'applic', 'appreci', 'apprehens', 'appropri', 'approx', 'approxim', 'april', 'area', 'aris', 'around', 'arround', 'asap', 'ask', 'ass', 'assist', 'assum', 'atleast', 'attach', 'attack', 'attempt', 'attend', 'aussi', 'australia', 'australian', 'author', 'auto', 'automat', 'avail', 'averag', 'avoid', 'aw', 'awar', 'away', 'awesom', 'awsom', 'back', 'background', 'backwardli', 'bad', 'badli', 'ball', 'base', 'basic', 'basicli', 'batt', 'batteri', 'baxk', 'bayteri', 'bear', 'beaten', 'becom', 'beep', 'begin', 'behind', 'beleav', 'believ', 'benefici', 'benefit', 'besid', 'best', 'better', 'beyondblu', 'big', 'bigger', 'biggest', 'bike', 'bill', 'birdsvil', 'bit', 'blame', 'blank', 'blatant', 'blend', 'bless', 'bloat', 'block', 'blood', 'blue', 'blurb', 'board', 'bom', 'bomb', 'book', 'bother', 'bottom', 'bought', 'bound', 'box', 'bravo', 'break', 'breakdown', 'breath', 'brilliant', 'bring', 'broke', 'broken', 'btw', 'bug', 'bugger', 'build', 'bunch', 'bush', 'bushfir', 'bushland', 'bushwalk', 'busi', 'button', 'c', 'cabl', 'cach', 'cadet', 'cal', 'call', 'caller', 'came', 'camp', 'cancel', 'cannot', 'cant', 'capabl', 'car', 'caravan', 'card', 'care', 'caresport', 'carri', 'carrier', 'case', 'caus', 'caution', 'cell', 'center', 'centr', 'certain', 'certif', 'challeng', 'chanc', 'chang', 'channel', 'charact', 'charg', 'charli', 'check', 'cheer', 'chequ', 'chest', 'child', 'children', 'chopper', 'chri', 'circumst', 'citi', 'clarif', 'class', 'claus', 'clean', 'clear', 'cleari', 'clearli', 'click', 'client', 'clipboard', 'clitgo', 'close', 'closer', 'closest', 'co', 'coast', 'code', 'collaps', 'colour', 'combin', 'come', 'comfort', 'command', 'comment', 'common', 'commun', 'compani', 'compat', 'compel', 'compens', 'complain', 'complaint', 'complet', 'completley', 'complex', 'comprehens', 'compress', 'compulseri', 'compulsori', 'concept', 'concern', 'concert', 'concis', 'condit', 'confid', 'conform', 'confus', 'congratul', 'connect', 'conserv', 'consid', 'constantli', 'contact', 'contain', 'content', 'continu', 'control', 'convert', 'convinc', 'cood', 'coodin', 'cool', 'coord', 'coordiat', 'coordin', 'cop', 'copi', 'copypast', 'cord', 'cordin', 'core', 'corner', 'correct', 'correctli', 'cost', 'could', 'couldnt', 'council', 'count', 'countri', 'cours', 'cov19', 'cover', 'coverag', 'covid', 'cpr', 'crap', 'crash', 'creat', 'creation', 'credit', 'crime', 'criteria', 'critic', 'cross', 'crossstreet', 'crowd', 'crt', 'culdesac', 'current', 'cut', 'cycl', 'dad', 'damag', 'danger', 'dark', 'darwin', 'data', 'date', 'daughter', 'day', 'dead', 'deadli', 'deaf', 'death', 'decid', 'declar', 'deed', 'deem', 'deep', 'default', 'definit', 'degrad', 'degre', 'dehydr', 'delay', 'delet', 'demand', 'dementia', 'demijohn', 'demonstrar', 'dens', 'depend', 'deplet', 'describ', 'design', 'desir', 'desper', 'despit', 'detail', 'detect', 'determin', 'dev', 'develop', 'devic', 'dial', 'dialer', 'diall', 'didnt', 'die', 'diff', 'differ', 'difficult', 'difficulti', 'digit', 'dillon', 'direct', 'directli', 'disabl', 'disappear', 'disappoint', 'disclaim', 'discov', 'disori', 'dispatch', 'display', 'dissapoint', 'distanc', 'distinct', 'distress', 'ditch', 'doctor', 'doesnt', 'dogwalk', 'done', 'done1010', 'dont', 'door', 'dosent', 'dot', 'download', 'downsid', 'downstair', 'drain', 'dramat', 'drink', 'drive', 'drop', 'drug', 'due', 'duplex', 'duress', 'dw', 'dwell', 'e', 'earlier', 'earth', 'easi', 'easier', 'easili', 'edg', 'educ', 'effect', 'effort', 'eg', 'either', 'elderli', 'els', 'em', 'email', 'embed', 'emerg', 'emergencycrisi', 'emergengi', 'enabl', 'encourag', 'end', 'engin', 'english', 'enhanc', 'enlarg', 'enough', 'ensur', 'enter', 'entiti', 'entranc', 'entri', 'er', 'ere', 'error', 'especi', 'ess', 'essenti', 'estat', 'estim', 'et', 'etc', 'evacu', 'even', 'event', 'ever', 'everi', 'everseen', 'everybodi', 'everyday', 'everyon', 'everyth', 'everytim', 'everywher', 'exact', 'exactli', 'exampl', 'excel', 'excus', 'execut', 'exist', 'exit', 'expect', 'expedit', 'experi', 'experienc', 'expert', 'explain', 'explor', 'express', 'extens', 'extract', 'extrem', 'eyeglass', 'eyesight', 'f', 'fabul', 'facebook', 'facil', 'fact', 'fade', 'fail', 'failz', 'fair', 'fairli', 'famili', 'familiar', 'fantast', 'far', 'farm', 'fast', 'faster', 'father', 'fault', 'feasibl', 'featur', 'feedback', 'feel', 'felt', 'fenc', 'ferrari', 'field', 'fill', 'final', 'find', 'fine', 'fingertip', 'fire', 'firefight', 'first', 'fit', 'five', 'fix', 'fk', 'flare', 'flat', 'flaw', 'flawlessli', 'flight', 'fluent', 'folk', 'follow', 'font', 'foot', 'footbal', 'forc', 'forget', 'forgot', 'former', 'forth', 'fortun', 'forward', 'found', 'fraca', 'frame', 'free', 'freind', 'fridg', 'friend', 'frill', 'frnsw', 'front', 'froze', 'frustrat', 'frustratingli', 'fulfil', 'full', 'function', 'fund', 'fyi', 'g3', 'g4', 'ga', 'galaxi', 'garmin', 'gasp', 'gate', 'gave', 'gener', 'geniu', 'genuin', 'geographi', 'geographicli', 'geotag', 'get', 'girl', 'girlfriend', 'give', 'given', 'glad', 'glass', 'glitch', 'go', 'goe', 'gold', 'good', 'googl', 'googlemap', 'got', 'govern', 'gp', 'gpswifi', 'gr8', 'grand', 'great', 'greatest', 'greatwel', 'grey', 'group', 'grrrr', 'guard', 'guess', 'guesswork', 'guid', 'guy', 'guysth', 'hadnt', 'half', 'hand', 'handheld', 'handi', 'hang', 'happen', 'happi', 'hard', 'hardli', 'hate', 'hav', 'have', 'havent', 'head', 'health', 'heart', 'heavili', 'held', 'helen', 'helicopt', 'hell', 'help', 'helplin', 'helpul', 'hi', 'high', 'highli', 'highlight', 'highway', 'hill', 'histori', 'hit', 'hold', 'home', 'hope', 'hopeless', 'hospit', 'hostag', 'hot', 'hotlin', 'hour', 'hous', 'howev', 'htc', 'hubbi', 'huge', 'hundr', 'huntli', 'husband', 'i22', 'iam', 'ice', 'icon', 'id', 'idalia', 'idea', 'ideal', 'identifi', 'ideo', 'ie', 'ignor', 'iii', 'ill', 'illeg', 'illustr', 'im', 'imagin', 'immedi', 'impact', 'impair', 'implement', 'import', 'importantli', 'imposs', 'impress', 'improv', 'inaccur', 'incapasit', 'incas', 'incid', 'includ', 'inclus', 'incom', 'inconsist', 'incorrect', 'incorrectli', 'increas', 'incred', 'inde', 'indemnif', 'indemnifi', 'indic', 'indispens', 'individu', 'indoor', 'industri', 'ine', 'inevit', 'info', 'inform', 'initi', 'injur', 'injuri', 'innov', 'input', 'instal', 'installeduninstal', 'installsuninstal', 'instant', 'instantli', 'instead', 'instruct', 'instructor', 'integr', 'intellig', 'interfac', 'intermitt', 'internet', 'intro', 'introduct', 'invalu', 'invent', 'io', 'iphon', 'isnt', 'issu', 'item', 'itjust', 'itll', 'ive', 'j7', 'jan', 'job', 'john', 'joke', 'keep', 'kept', 'kid', 'kill', 'kilomet', 'kin', 'kindergarten', 'kindli', 'kingaroy', 'klm', 'km', 'knew', 'know', 'knowledg', 'known', 'ks', 'label', 'lack', 'lake', 'lakeview', 'land', 'landmark', 'larg', 'larger', 'last', 'lat', 'late', 'latest', 'latitud', 'latlong', 'launch', 'law', 'layout', 'lead', 'learnt', 'least', 'legibl', 'less', 'lessen', 'let', 'letter', 'level', 'liabil', 'licenc', 'licens', 'lie', 'life', 'lifeguard', 'lifelin', 'lifesav', 'like', 'likewis', 'limit', 'line', 'link', 'list', 'listen', 'liter', 'littl', 'live', 'livw', 'load', 'local', 'locat', 'locationgoogl', 'lock', 'lodgment', 'log', 'login', 'logo', 'lol', 'lone', 'long', 'longer', 'longitud', 'longlat', 'look', 'loss', 'lost', 'lot', 'loud', 'loung', 'lousi', 'love', 'low', 'lower', 'lucki', 'luckili', 'm1', 'macquari', 'made', 'magazin', 'main', 'major', 'make', 'maker', 'manag', 'mani', 'manual', 'map', 'maploc', 'mark', 'marko', 'marshmallow', 'massiv', 'match', 'mate', 'matter', 'may', 'mayb', 'mb', 'mean', 'meaningl', 'meaningless', 'meant', 'med', 'media', 'medic', 'medium', 'member', 'mental', 'mention', 'menu', 'mess', 'messag', 'messeng', 'metr', 'metronom', 'middl', 'might', 'million', 'min', 'mind', 'mine', 'minimum', 'minu', 'minut', 'miscommun', 'mislead', 'miss', 'misunderstand', 'mix', 'mobil', 'mode', 'modern', 'modifi', 'moment', 'money', 'monitor', 'month', 'morn', 'mostli', 'moto', 'motog7', 'motor', 'motorcycl', 'mototola', 'move', 'much', 'multi', 'multipl', 'murphi', 'must', 'n', 'name', 'nation', 'natur', 'near', 'nearbi', 'nearest', 'necessari', 'need', 'needsmor', 'neg', 'neighborhood', 'neighbour', 'neil', 'nessesari', 'network', 'never', 'new', 'newer', 'newest', 'news', 'newspap', 'newupdatedappwillnotopen', 'next', 'nextdoor', 'nexu', 'nexus6p', 'nice', 'nicetohav', 'night', 'nlm', 'nomad', 'non', 'none', 'nong', 'noon', 'normal', 'note', 'noth', 'notic', 'notif', 'notori', 'nougat', 'nov', 'novemb', 'nowher', 'nowth', 'nr', 'nrma', 'nsw', 'nuget', 'number', 'numberstri', 'numer', 'observ', 'obviou', 'obvious', 'occas', 'ocean', 'oen', 'offic', 'offroad', 'often', 'ok', 'old', 'older', 'olymp', 'one', 'oner', 'onlin', 'onoff', 'ont', 'onto', 'open', 'openstreetmap', 'oper', 'opinion', 'opportun', 'opposit', 'option', 'orang', 'ord', 'order', 'organis', 'origin', 'os', 'other', 'otherwis', 'outback', 'outdoor', 'outstand', 'outway', 'overdu', 'overlay', 'overnight', 'oversight', 'oz', 'p', 'page', 'panic', 'parent', 'park', 'part', 'partial', 'particularli', 'partner', 'pass', 'password', 'past', 'path', 'pathet', 'patient', 'paul', 'pay', 'peac', 'peopl', 'peopleor', 'percent', 'perfect', 'perfectli', 'perform', 'perhap', 'permiss', 'person', 'personnel', 'phone', 'phonegreat', 'phonehop', 'phonestop', 'photo', 'photosmediafil', 'phrase', 'physic', 'pick', 'pictur', 'piec', 'pin', 'pinch', 'pinpoint', 'place', 'plain', 'plan', 'plater', 'play', 'playstor', 'pleas', 'plenti', 'plu', 'pocket', 'point', 'pointless', 'poison', 'polic', 'polici', 'poll', 'pool', 'poor', 'pop', 'posit', 'possibl', 'post', 'potenti', 'power', 'pre', 'preciou', 'precis', 'prefer', 'pregnant', 'preinstal', 'preload', 'preseason', 'press', 'presum', 'pretti', 'prevent', 'preview', 'previou', 'previous', 'prewritten', 'primari', 'print', 'prior', 'probabl', 'problem', 'problemat', 'process', 'product', 'program', 'promis', 'prompt', 'promptli', 'prone', 'pronounc', 'properli', 'properti', 'proprietari', 'protocol', 'prove', 'proven', 'provid', 'public', 'purpos', 'push', 'put', 'putti', 'qa', 'queensland', 'queri', 'question', 'quick', 'quicker', 'quickley', 'quickli', 'race', 'radio', 'ran', 'random', 'ranger', 'rare', 'rate', 'rather', 'ration', 'rd', 'reach', 'read', 'readabl', 'readi', 'readili', 'real', 'realli', 'reallif', 'realtim', 'rearrang', 'reason', 'reassur', 'receiv', 'recent', 'recept', 'recommend', 'record', 'rectifi', 'red', 'redmi', 'reduc', 'refer', 'refin', 'reflect', 'refresh', 'refus', 'regard', 'regardless', 'region', 'regist', 'registr', 'reinforc', 'reinstal', 'reinstat', 'rel', 'relaunch', 'relay', 'releas', 'relev', 'reli', 'reliabl', 'reload', 'rememb', 'remind', 'remot', 'remov', 'render', 'renew', 'repair', 'repeat', 'replac', 'repli', 'report', 'reposit', 'request', 'requir', 'rescu', 'residenti', 'resolv', 'resourc', 'respond', 'respons', 'restart', 'restor', 'resubmit', 'result', 'resuscit', 'revert', 'review', 'revis', 'reward', 'ride', 'riderthi', 'ridicul', 'right', 'ring', 'rip', 'road', 'robust', 'rock', 'roll', 'rollov', 'room', 'rope', 'rough', 'row', 'run', 'rural', 's2', 's3', 's4', 's5', 's6', 's7', 's9', 'sack', 'sad', 'sadli', 'safer', 'safeti', 'said', 'sailor', 'samsung', 'samsungalaxi', 'samung', 'satellit', 'save', 'saver', 'saw', 'say', 'scene', 'scheme', 'school', 'score', 'scrambl', 'screen', 'screenshot', 'scroll', 'scrub', 'se', 'seach', 'search', 'season', 'second', 'section', 'secur', 'see', 'seem', 'seen', 'seiz', 'select', 'self', 'send', 'senior', 'sens', 'sent', 'seriou', 'serious', 'serv', 'servic', 'session', 'set', 'settingsgtapp', 'setup', 'sever', 'shame', 'share', 'shock', 'shoe', 'show', 'showin', 'shown', 'shut', 'side', 'sight', 'sign', 'signal', 'significantli', 'silent', 'sim', 'similar', 'simpl', 'simplest', 'simpli', 'sinc', 'singl', 'sister', 'sit', 'site', 'situat', 'six', 'size', 'skool', 'slighlti', 'slightest', 'slightli', 'slow', 'sm', 'small', 'smallest', 'smart', 'smarter', 'smartest', 'smartphon', 'smile', 'snakebit', 'snow', 'so', 'social', 'solar', 'sold', 'solv', 'someon', 'someth', 'sometim', 'somewher', 'soo', 'soon', 'sorri', 'sort', 'sound', 'sourc', 'south', 'space', 'spam', 'spare', 'speak', 'specialist', 'specif', 'specifi', 'speech', 'speedi', 'sport', 'spot', 'spring', 'squar', 'st', 'staff', 'stake', 'stand', 'standard', 'standbi', 'star', 'start', 'startup', 'state', 'station', 'stay', 'still', 'stop', 'stope', 'stopp', 'stopper', 'storag', 'stori', 'straight', 'strang', 'street', 'stress', 'stretch', 'string', 'stroke', 'strongest', 'struggl', 'student', 'stuff', 'subject', 'suburb', 'suburbia', 'success', 'suck', 'suddenli', 'suffer', 'suggest', 'suicid', 'sumsung', 'sunglass', 'super', 'supervisor', 'support', 'suppos', 'sure', 'surround', 'surviv', 'swan', 'swappabl', 'swing', 'sydney', 'symbol', 'synchronis', 'system', 'tab', 'tablet', 'tad', 'take', 'talk', 'tap', 'task', 'tasker', 'taught', 'tc', 'teach', 'teacher', 'team', 'tech', 'technolog', 'teenag', 'telephon', 'telephonist', 'tell', 'telstra', 'term', 'termscondit', 'terribl', 'terrif', 'test', 'text', 'thank', 'thankyou', 'that', 'there', 'therefor', 'thing', 'think', 'thiswhoev', 'though', 'thought', 'thousand', 'thr', 'threaten', 'thredbo', 'three', 'throttl', 'throughli', 'throughout', 'thu', 'thumb', 'time', 'tini', 'today', 'togeth', 'toinstal', 'told', 'tool', 'top', 'total', 'touch', 'town', 'township', 'track', 'traffic', 'train', 'transcrib', 'translat', 'transmit', 'travel', 'travll', 'tri', 'triangul', 'tripl', 'trivial', 'troubl', 'true', 'truli', 'truncat', 'trust', 'trusti', 'tsc', 'tune', 'turn', 'turnr', 'tv', 'tweak', 'twice', 'two', 'txt', 'type', 'u', 'ugli', 'ui', 'unabl', 'unclear', 'understand', 'unexpect', 'unfamiliar', 'unfortun', 'uninstal', 'uninstalledjust', 'unit', 'unless', 'unlock', 'unnecessari', 'unread', 'unrespons', 'unsaf', 'unstabl', 'unsuccess', 'unsur', 'until', 'untrain', 'up', 'updat', 'updatewil', 'upgrad', 'upload', 'upon', 'upstair', 'upto', 'ur', 'urg', 'urgent', 'us', 'usabl', 'usag', 'use', 'useful', 'useless', 'user', 'userfriendli', 'usual', 'usurp', 'utterli', 'v', 'v7', 'vacant', 'valuabl', 'van', 'vari', 'variabl', 'varieti', 'variou', 'vastli', 'vehicl', 'verbal', 'version', 'via', 'vic', 'vicpol', 'victoria', 'view', 'viewaccess', 'villian', 'visibl', 'vision', 'visual', 'vital', 'voic', 'volunt', 'vote', 'vwt', 'w', 'w3w', 'wa', 'wait', 'walk', 'walkway', 'wall', 'wallet', 'want', 'warden', 'warn', 'wasnt', 'wast', 'wat', 'watch', 'water', 'waterbrook', 'way', 'waze', 'weak', 'wealthi', 'wear', 'web', 'wed', 'wee', 'week', 'weekend', 'weer', 'welcom', 'welfar', 'well', 'went', 'west', 'western', 'weve', 'what', 'what3word', 'whatsapp', 'wheelchair', 'whereabout', 'whilst', 'wide', 'widget', 'wife', 'wifi', 'wildli', 'wildlid', 'winter', 'wireless', 'wish', 'with', 'within', 'without', 'wonder', 'wont', 'word', 'work', 'workaround', 'worker', 'worri', 'worrisom', 'wors', 'worth', 'worthwhil', 'would', 'wouldb', 'wouldnt', 'wouod', 'wow', 'write', 'writren', 'written', 'wrong', 'wrote', 'wtf', 'wwest', 'x', 'xiaomi', 'xo', 'xstreet', 'ya', 'ye', 'year', 'yet', 'youll', 'young', 'your', 'yr', 'zero', '', '', '', '', '', 'accident', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3121f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 144)\n"
     ]
    }
   ],
   "source": [
    "df_sample = df[0:10]\n",
    "cv2 = CountVectorizer(analyzer=clean_text)\n",
    "#The bellow code returns an error, (why?) thats why I replaced it with another code\n",
    "# x = cv2.fit_transform(df_sample['REVIEW_clean_tokenized_no_sw_lemmatized'])\n",
    "x1 = cv2.fit_transform(df_sample['REVIEW'])\n",
    "print(x.shape)\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c3eb381e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>3</th>\n",
       "      <th>50km</th>\n",
       "      <th>access</th>\n",
       "      <th>actual</th>\n",
       "      <th>addit</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>alpha</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>waze</th>\n",
       "      <th>welfar</th>\n",
       "      <th>well</th>\n",
       "      <th>within</th>\n",
       "      <th>wont</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xstreet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  3  50km  access  actual  addit  address  age  alpha  also  ...  waze  \\\n",
       "0    0  0     0       0       0      0        0    0      0     0  ...     0   \n",
       "1    0  0     0       0       0      0        0    0      0     0  ...     0   \n",
       "2    1  0     0       0       0      0        1    1      1     0  ...     0   \n",
       "3    0  0     0       0       0      0        0    0      0     0  ...     0   \n",
       "4    0  0     1       0       1      0        1    0      0     0  ...     1   \n",
       "5    0  0     0       1       0      1        0    0      0     1  ...     0   \n",
       "6    0  0     0       0       0      0        0    0      0     0  ...     0   \n",
       "7    0  0     0       0       0      0        0    0      0     0  ...     0   \n",
       "8    0  1     0       0       0      0        0    0      0     0  ...     0   \n",
       "9    0  0     0       0       0      0        1    0      0     0  ...     0   \n",
       "\n",
       "   welfar  well  within  wont  word  work  would  wrong  xstreet  \n",
       "0       0     0       0     0     0     0      0      0        0  \n",
       "1       0     0       0     0     0     0      0      0        0  \n",
       "2       1     1       1     0     1     0      0      0        1  \n",
       "3       0     0       0     0     0     1      0      0        0  \n",
       "4       0     0       0     1     0     0      1      1        0  \n",
       "5       0     0       0     0     2     0      0      0        0  \n",
       "6       0     0       0     0     0     0      0      0        0  \n",
       "7       0     0       0     0     0     0      0      0        0  \n",
       "8       0     0       0     0     1     2      0      0        0  \n",
       "9       0     0       0     0     0     0      0      1        0  \n",
       "\n",
       "[10 rows x 144 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.DataFrame(x1.toarray(), columns=cv2.get_feature_names_out())\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cf1d3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7d1de721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_clean</th>\n",
       "      <th>REVIEW_clean_tokenized</th>\n",
       "      <th>REVIEW_no_sw</th>\n",
       "      <th>REVIEW_clean_tokenized_no_sw_stemmed</th>\n",
       "      <th>REVIEW_clean_tokenized_no_sw_lemmatized</th>\n",
       "      <th>REVIEW_another_clean_tx</th>\n",
       "      <th>REVIEW_another_clean_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>[we]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>handy to have</td>\n",
       "      <td>[handy, to, have]</td>\n",
       "      <td>[handy]</td>\n",
       "      <td>[handi]</td>\n",
       "      <td>[handy]</td>\n",
       "      <td>handi</td>\n",
       "      <td>handi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>rendering assistance to a lone dehydrated aged sailor at water's edge reporting as a \"concern for welfare\". frustratingly with no road and no x-st...</td>\n",
       "      <td>rendering assistance to a lone dehydrated aged sailor at waters edge reporting as a concern for welfare frustratingly with no road and no xstreet ...</td>\n",
       "      <td>[rendering, assistance, to, a, lone, dehydrated, aged, sailor, at, waters, edge, reporting, as, a, concern, for, welfare, frustratingly, with, no,...</td>\n",
       "      <td>[rendering, assistance, lone, dehydrated, aged, sailor, waters, edge, reporting, concern, welfare, frustratingly, road, xstreet, pass, 000, simple...</td>\n",
       "      <td>[render, assist, lone, dehydr, age, sailor, water, edg, report, concern, welfar, frustratingli, road, xstreet, pass, 000, simpl, say, alpha, bravo...</td>\n",
       "      <td>[rendering, assistance, lone, dehydrated, aged, sailor, water, edge, reporting, concern, welfare, frustratingly, road, xstreet, pas, 000, simple, ...</td>\n",
       "      <td>render assist lone dehydr age sailor water edg report concern welfar frustratingli road xstreet pass 000 simpl say alpha bravo charli gp three wor...</td>\n",
       "      <td>render assist lone dehydr age sailor water edg report concern welfar frustratingli road xstreet pass 000 simpl say alpha bravo charli gp three wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>great app. sister put me onto it. shame there is no way to update the home location as a gps location does work for when you are multi level resid...</td>\n",
       "      <td>great app sister put me onto it shame there is no way to update the home location as a gps location does work for when you are multi level residen...</td>\n",
       "      <td>[great, app, sister, put, me, onto, it, shame, there, is, no, way, to, update, the, home, location, as, a, gps, location, does, work, for, when, y...</td>\n",
       "      <td>[great, app, sister, put, onto, shame, way, update, home, location, gps, location, work, multi, level, residential, building]</td>\n",
       "      <td>[great, app, sister, put, onto, shame, way, updat, home, locat, gp, locat, work, multi, level, residenti, build]</td>\n",
       "      <td>[great, app, sister, put, onto, shame, way, update, home, location, gps, location, work, multi, level, residential, building]</td>\n",
       "      <td>great app sister put onto shame way updat home locat gp locat work multi level residenti build</td>\n",
       "      <td>great app sister put onto shame way updat home locat gp locat work multi level residenti build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>dangerous app. it thinks i am at my old address, some 50km from where i actually am. gps won't update. waze picks up my location correctly so it i...</td>\n",
       "      <td>dangerous app it thinks i am at my old address some 50km from where i actually am gps wont update waze picks up my location correctly so it isnt a...</td>\n",
       "      <td>[dangerous, app, it, thinks, i, am, at, my, old, address, some, 50km, from, where, i, actually, am, gps, wont, update, waze, picks, up, my, locati...</td>\n",
       "      <td>[dangerous, app, thinks, old, address, 50km, actually, gps, wont, update, waze, picks, location, correctly, isnt, gps, phone, issue, app, dangerou...</td>\n",
       "      <td>[danger, app, think, old, address, 50km, actual, gp, wont, updat, waze, pick, locat, correctli, isnt, gp, phone, issu, app, danger, could, send, e...</td>\n",
       "      <td>[dangerous, app, think, old, address, 50km, actually, gps, wont, update, waze, pick, location, correctly, isnt, gps, phone, issue, app, dangerous,...</td>\n",
       "      <td>danger app think old address 50km actual gp wont updat waze pick locat correctli isnt gp phone issu app danger could send emerg servic wrong locat...</td>\n",
       "      <td>danger app think old address 50km actual gp wont updat waze pick locat correctli isnt gp phone issu app danger could send emerg servic wrong locat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SENTIMENT  \\\n",
       "0   neutral   \n",
       "1   neutral   \n",
       "2  negative   \n",
       "3  negative   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                  REVIEW  \\\n",
       "0                                                                                                                                                     we   \n",
       "1                                                                                                                                          handy to have   \n",
       "2  rendering assistance to a lone dehydrated aged sailor at water's edge reporting as a \"concern for welfare\". frustratingly with no road and no x-st...   \n",
       "3  great app. sister put me onto it. shame there is no way to update the home location as a gps location does work for when you are multi level resid...   \n",
       "4  dangerous app. it thinks i am at my old address, some 50km from where i actually am. gps won't update. waze picks up my location correctly so it i...   \n",
       "\n",
       "                                                                                                                                            REVIEW_clean  \\\n",
       "0                                                                                                                                                     we   \n",
       "1                                                                                                                                          handy to have   \n",
       "2  rendering assistance to a lone dehydrated aged sailor at waters edge reporting as a concern for welfare frustratingly with no road and no xstreet ...   \n",
       "3  great app sister put me onto it shame there is no way to update the home location as a gps location does work for when you are multi level residen...   \n",
       "4  dangerous app it thinks i am at my old address some 50km from where i actually am gps wont update waze picks up my location correctly so it isnt a...   \n",
       "\n",
       "                                                                                                                                  REVIEW_clean_tokenized  \\\n",
       "0                                                                                                                                                   [we]   \n",
       "1                                                                                                                                      [handy, to, have]   \n",
       "2  [rendering, assistance, to, a, lone, dehydrated, aged, sailor, at, waters, edge, reporting, as, a, concern, for, welfare, frustratingly, with, no,...   \n",
       "3  [great, app, sister, put, me, onto, it, shame, there, is, no, way, to, update, the, home, location, as, a, gps, location, does, work, for, when, y...   \n",
       "4  [dangerous, app, it, thinks, i, am, at, my, old, address, some, 50km, from, where, i, actually, am, gps, wont, update, waze, picks, up, my, locati...   \n",
       "\n",
       "                                                                                                                                            REVIEW_no_sw  \\\n",
       "0                                                                                                                                                     []   \n",
       "1                                                                                                                                                [handy]   \n",
       "2  [rendering, assistance, lone, dehydrated, aged, sailor, waters, edge, reporting, concern, welfare, frustratingly, road, xstreet, pass, 000, simple...   \n",
       "3                          [great, app, sister, put, onto, shame, way, update, home, location, gps, location, work, multi, level, residential, building]   \n",
       "4  [dangerous, app, thinks, old, address, 50km, actually, gps, wont, update, waze, picks, location, correctly, isnt, gps, phone, issue, app, dangerou...   \n",
       "\n",
       "                                                                                                                    REVIEW_clean_tokenized_no_sw_stemmed  \\\n",
       "0                                                                                                                                                     []   \n",
       "1                                                                                                                                                [handi]   \n",
       "2  [render, assist, lone, dehydr, age, sailor, water, edg, report, concern, welfar, frustratingli, road, xstreet, pass, 000, simpl, say, alpha, bravo...   \n",
       "3                                       [great, app, sister, put, onto, shame, way, updat, home, locat, gp, locat, work, multi, level, residenti, build]   \n",
       "4  [danger, app, think, old, address, 50km, actual, gp, wont, updat, waze, pick, locat, correctli, isnt, gp, phone, issu, app, danger, could, send, e...   \n",
       "\n",
       "                                                                                                                 REVIEW_clean_tokenized_no_sw_lemmatized  \\\n",
       "0                                                                                                                                                     []   \n",
       "1                                                                                                                                                [handy]   \n",
       "2  [rendering, assistance, lone, dehydrated, aged, sailor, water, edge, reporting, concern, welfare, frustratingly, road, xstreet, pas, 000, simple, ...   \n",
       "3                          [great, app, sister, put, onto, shame, way, update, home, location, gps, location, work, multi, level, residential, building]   \n",
       "4  [dangerous, app, think, old, address, 50km, actually, gps, wont, update, waze, pick, location, correctly, isnt, gps, phone, issue, app, dangerous,...   \n",
       "\n",
       "                                                                                                                                 REVIEW_another_clean_tx  \\\n",
       "0                                                                                                                                                          \n",
       "1                                                                                                                                                  handi   \n",
       "2  render assist lone dehydr age sailor water edg report concern welfar frustratingli road xstreet pass 000 simpl say alpha bravo charli gp three wor...   \n",
       "3                                                         great app sister put onto shame way updat home locat gp locat work multi level residenti build   \n",
       "4  danger app think old address 50km actual gp wont updat waze pick locat correctli isnt gp phone issu app danger could send emerg servic wrong locat...   \n",
       "\n",
       "                                                                                                                                REVIEW_another_clean_txt  \n",
       "0                                                                                                                                                         \n",
       "1                                                                                                                                                  handi  \n",
       "2  render assist lone dehydr age sailor water edg report concern welfar frustratingli road xstreet pass 000 simpl say alpha bravo charli gp three wor...  \n",
       "3                                                         great app sister put onto shame way updat home locat gp locat work multi level residenti build  \n",
       "4  danger app think old address 50km actual gp wont updat waze pick locat correctli isnt gp phone issu app danger could send emerg servic wrong locat...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We gonna use another cleaning function\n",
    "def another_clean_text(txt):\n",
    "    txt = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    tokens = re.split('\\W+', txt)\n",
    "    #because n-gram expect sentences we add \" \".join()\n",
    "    txt = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return txt\n",
    "\n",
    "# going to create a new column to pass the another_clean_function\n",
    "df['REVIEW_another_clean_txt'] = df['REVIEW'].apply(lambda x: another_clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "906490af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751, 14615)\n"
     ]
    }
   ],
   "source": [
    "#we are passing it to the CountVectorizer(why?)\n",
    "cv3 = CountVectorizer(ngram_range=(3,4))\n",
    "x3 = cv3.fit_transform(df['REVIEW_another_clean_txt'])\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e35dab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 344)\n"
     ]
    }
   ],
   "source": [
    "df_sample = df[0:10]\n",
    "cv32 = CountVectorizer(ngram_range=(3,4))\n",
    "x3 = cv32.fit_transform(df_sample['REVIEW_another_clean_txt'])\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fbee02a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000 simpl say</th>\n",
       "      <th>000 simpl say alpha</th>\n",
       "      <th>50km actual gp</th>\n",
       "      <th>50km actual gp wont</th>\n",
       "      <th>access menu may</th>\n",
       "      <th>access menu may also</th>\n",
       "      <th>actual gp wont</th>\n",
       "      <th>actual gp wont updat</th>\n",
       "      <th>addit servic access</th>\n",
       "      <th>addit servic access menu</th>\n",
       "      <th>...</th>\n",
       "      <th>work multi level</th>\n",
       "      <th>work multi level residenti</th>\n",
       "      <th>work right sinc</th>\n",
       "      <th>work right sinc last</th>\n",
       "      <th>would safer app</th>\n",
       "      <th>would safer app use</th>\n",
       "      <th>wrong locat would</th>\n",
       "      <th>wrong locat would safer</th>\n",
       "      <th>xstreet pass 000</th>\n",
       "      <th>xstreet pass 000 simpl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000 simpl say  000 simpl say alpha  50km actual gp  50km actual gp wont  \\\n",
       "0              0                    0               0                    0   \n",
       "1              0                    0               0                    0   \n",
       "2              1                    1               0                    0   \n",
       "3              0                    0               0                    0   \n",
       "4              0                    0               1                    1   \n",
       "5              0                    0               0                    0   \n",
       "6              0                    0               0                    0   \n",
       "7              0                    0               0                    0   \n",
       "8              0                    0               0                    0   \n",
       "9              0                    0               0                    0   \n",
       "\n",
       "   access menu may  access menu may also  actual gp wont  \\\n",
       "0                0                     0               0   \n",
       "1                0                     0               0   \n",
       "2                0                     0               0   \n",
       "3                0                     0               0   \n",
       "4                0                     0               1   \n",
       "5                1                     1               0   \n",
       "6                0                     0               0   \n",
       "7                0                     0               0   \n",
       "8                0                     0               0   \n",
       "9                0                     0               0   \n",
       "\n",
       "   actual gp wont updat  addit servic access  addit servic access menu  ...  \\\n",
       "0                     0                    0                         0  ...   \n",
       "1                     0                    0                         0  ...   \n",
       "2                     0                    0                         0  ...   \n",
       "3                     0                    0                         0  ...   \n",
       "4                     1                    0                         0  ...   \n",
       "5                     0                    1                         1  ...   \n",
       "6                     0                    0                         0  ...   \n",
       "7                     0                    0                         0  ...   \n",
       "8                     0                    0                         0  ...   \n",
       "9                     0                    0                         0  ...   \n",
       "\n",
       "   work multi level  work multi level residenti  work right sinc  \\\n",
       "0                 0                           0                0   \n",
       "1                 0                           0                0   \n",
       "2                 0                           0                0   \n",
       "3                 1                           1                0   \n",
       "4                 0                           0                0   \n",
       "5                 0                           0                0   \n",
       "6                 0                           0                0   \n",
       "7                 0                           0                0   \n",
       "8                 0                           0                1   \n",
       "9                 0                           0                0   \n",
       "\n",
       "   work right sinc last  would safer app  would safer app use  \\\n",
       "0                     0                0                    0   \n",
       "1                     0                0                    0   \n",
       "2                     0                0                    0   \n",
       "3                     0                0                    0   \n",
       "4                     0                1                    1   \n",
       "5                     0                0                    0   \n",
       "6                     0                0                    0   \n",
       "7                     0                0                    0   \n",
       "8                     1                0                    0   \n",
       "9                     0                0                    0   \n",
       "\n",
       "   wrong locat would  wrong locat would safer  xstreet pass 000  \\\n",
       "0                  0                        0                 0   \n",
       "1                  0                        0                 0   \n",
       "2                  0                        0                 1   \n",
       "3                  0                        0                 0   \n",
       "4                  1                        1                 0   \n",
       "5                  0                        0                 0   \n",
       "6                  0                        0                 0   \n",
       "7                  0                        0                 0   \n",
       "8                  0                        0                 0   \n",
       "9                  0                        0                 0   \n",
       "\n",
       "   xstreet pass 000 simpl  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       1  \n",
       "3                       0  \n",
       "4                       0  \n",
       "5                       0  \n",
       "6                       0  \n",
       "7                       0  \n",
       "8                       0  \n",
       "9                       0  \n",
       "\n",
       "[10 rows x 344 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(x3.toarray(),columns=cv32.get_feature_names_out())\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bb775f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZER\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ff6906ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751, 1759)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer on data (same code as previous parts)\n",
    "cv1 = CountVectorizer(analyzer=clean_text)\n",
    "x=cv1.fit_transform(df['REVIEW'])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ad9615ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 144)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_sample = df[0:15]\n",
    "tfidf2 = TfidfVectorizer(analyzer= clean_text)\n",
    "x = tfidf2.fit_transform(data_sample['REVIEW'])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7a90f354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>3</th>\n",
       "      <th>50km</th>\n",
       "      <th>access</th>\n",
       "      <th>actual</th>\n",
       "      <th>addit</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>alpha</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>waze</th>\n",
       "      <th>welfar</th>\n",
       "      <th>well</th>\n",
       "      <th>within</th>\n",
       "      <th>wont</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xstreet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.138974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148454</td>\n",
       "      <td>0.126199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.331904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000         3      50km    access    actual     addit   address  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.138974  0.000000  0.000000  0.000000  0.000000  0.000000  0.103359   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.148454  0.000000  0.148454  0.000000  0.110409   \n",
       "5  0.000000  0.000000  0.000000  0.135713  0.000000  0.135713  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.195217  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.493002   \n",
       "\n",
       "        age     alpha      also  ...      waze    welfar      well    within  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.138974  0.138974  0.000000  ...  0.000000  0.138974  0.138974  0.138974   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  ...  0.148454  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.135713  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       wont      word      work     would     wrong   xstreet  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.103359  0.000000  0.000000  0.000000  0.138974  \n",
       "3  0.000000  0.000000  0.213899  0.000000  0.000000  0.000000  \n",
       "4  0.148454  0.000000  0.000000  0.148454  0.126199  0.000000  \n",
       "5  0.000000  0.201867  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8  0.000000  0.145188  0.331904  0.000000  0.000000  0.000000  \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.563507  0.000000  \n",
       "\n",
       "[10 rows x 144 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x.toarray(), columns = tfidf2.get_feature_names_out())\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7682803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c96b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
